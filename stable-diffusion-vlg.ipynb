{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Libraries and Paths\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport torch\nfrom transformers import AutoProcessor, BlipForConditionalGeneration\n\nimport os\nimport sys\nsys.path.append('/kaggle/input/sentence-transformers-222/sentence-transformers')\nfrom sentence_transformers import SentenceTransformer, models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-22T09:36:57.871656Z","iopub.execute_input":"2023-06-22T09:36:57.872066Z","iopub.status.idle":"2023-06-22T09:37:04.188286Z","shell.execute_reply.started":"2023-06-22T09:36:57.872027Z","shell.execute_reply":"2023-06-22T09:37:04.187360Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Setup Model","metadata":{}},{"cell_type":"code","source":"comp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')\nfolder_path = \"/kaggle/working/\" \nimage_files=[]\nfor dirname, _, filenames in os.walk('/kaggle/input/stable-diffusion-image-to-prompts/images/'):\n\n    for filename in sorted(filenames):\n\n        image_files.append(os.path.join(dirname, filename))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:04.194452Z","iopub.execute_input":"2023-06-22T09:37:04.195102Z","iopub.status.idle":"2023-06-22T09:37:04.226540Z","shell.execute_reply.started":"2023-06-22T09:37:04.195067Z","shell.execute_reply":"2023-06-22T09:37:04.225643Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"images = sorted(os.listdir(comp_path / 'images'))\nimgIds = [i.split('.')[0] for i in images]\n\nEMBEDDING_LENGTH = 384\neIds = list(range(EMBEDDING_LENGTH))\n\nimgId_eId = [\n    '_'.join(map(str, i)) for i in zip(\n        np.repeat(imgIds, EMBEDDING_LENGTH),\n        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:04.230003Z","iopub.execute_input":"2023-06-22T09:37:04.230608Z","iopub.status.idle":"2023-06-22T09:37:04.244913Z","shell.execute_reply.started":"2023-06-22T09:37:04.230574Z","shell.execute_reply":"2023-06-22T09:37:04.244040Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Calling Transformer Model","metadata":{}},{"cell_type":"code","source":"st_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:04.247987Z","iopub.execute_input":"2023-06-22T09:37:04.248293Z","iopub.status.idle":"2023-06-22T09:37:04.705896Z","shell.execute_reply.started":"2023-06-22T09:37:04.248270Z","shell.execute_reply":"2023-06-22T09:37:04.704827Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Diffusion Process through BLIP","metadata":{}},{"cell_type":"code","source":"processor = AutoProcessor.from_pretrained(\"/kaggle/input/salesforceblip-image-caption\")\n\nmodel = BlipForConditionalGeneration.from_pretrained(\"/kaggle/input/salesforceblip-image-caption\")\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:04.707429Z","iopub.execute_input":"2023-06-22T09:37:04.708368Z","iopub.status.idle":"2023-06-22T09:37:13.390213Z","shell.execute_reply.started":"2023-06-22T09:37:04.708328Z","shell.execute_reply":"2023-06-22T09:37:13.389376Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"BlipForConditionalGeneration(\n  (vision_model): BlipVisionModel(\n    (embeddings): BlipVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (encoder): BlipEncoder(\n      (layers): ModuleList(\n        (0-23): 24 x BlipEncoderLayer(\n          (self_attn): BlipAttention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (projection): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (mlp): BlipMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (text_decoder): BlipTextLMHeadModel(\n    (bert): BlipTextModel(\n      (embeddings): BlipTextEmbeddings(\n        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): BlipTextEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BlipTextLayer(\n            (attention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (crossattention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=1024, out_features=768, bias=True)\n                (value): Linear(in_features=1024, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): BlipTextIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BlipTextOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (cls): BlipTextOnlyMLMHead(\n      (predictions): BlipTextLMPredictionHead(\n        (transform): BlipTextPredictionHeadTransform(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (transform_act_fn): GELUActivation()\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def image_to_prompt(raw_image):\n    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n    out = model.generate(**inputs, max_new_tokens=32)\n    generated_prompt = processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n    return generated_prompt","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:13.394327Z","iopub.execute_input":"2023-06-22T09:37:13.396502Z","iopub.status.idle":"2023-06-22T09:37:13.403691Z","shell.execute_reply.started":"2023-06-22T09:37:13.396469Z","shell.execute_reply":"2023-06-22T09:37:13.402826Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"generated_prompts =[]\n\nfor idx, file in enumerate(tqdm(image_files, desc='Generating prompts')):\n\n\n    image = Image.open(file)\n\n    prompt = image_to_prompt(image)\n\n\n    generated_prompts.append(prompt)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:13.407810Z","iopub.execute_input":"2023-06-22T09:37:13.410081Z","iopub.status.idle":"2023-06-22T09:37:17.055428Z","shell.execute_reply.started":"2023-06-22T09:37:13.410040Z","shell.execute_reply":"2023-06-22T09:37:17.054439Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Generating prompts: 100%|██████████| 7/7 [00:03<00:00,  1.93it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Prompts generated","metadata":{}},{"cell_type":"code","source":"generated_prompts","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:17.056709Z","iopub.execute_input":"2023-06-22T09:37:17.057354Z","iopub.status.idle":"2023-06-22T09:37:17.064395Z","shell.execute_reply.started":"2023-06-22T09:37:17.057318Z","shell.execute_reply":"2023-06-22T09:37:17.063368Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['arafed view of a circular hole in the middle of a desert',\n 'a close up of a wooden plate with a swirl design on it',\n 'cartoon dinosaur with a piece of cheese in its mouth in a forest',\n 'there is a drawing of a robot holding a hammer',\n 'painting of a man with a lizard on his head and a lizard on his shoulder',\n 'arafed astronaut walking down a path in a park with cherry trees',\n 'there is a man standing in front of a counter with a pizza']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Embedding Calculation","metadata":{}},{"cell_type":"code","source":"\nprompt_embeddings = st_model.encode(generated_prompts).flatten()\nsubmission = pd.DataFrame(\n                index=imgId_eId,\n                data=prompt_embeddings,\n                columns=['val']).rename_axis('imgId_eId')","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:17.065658Z","iopub.execute_input":"2023-06-22T09:37:17.066672Z","iopub.status.idle":"2023-06-22T09:37:17.137558Z","shell.execute_reply.started":"2023-06-22T09:37:17.066639Z","shell.execute_reply":"2023-06-22T09:37:17.136640Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e20345f983b4c8cbb6524c46ca0c187"}},"metadata":{}}]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:17.139034Z","iopub.execute_input":"2023-06-22T09:37:17.139678Z","iopub.status.idle":"2023-06-22T09:37:17.155227Z","shell.execute_reply.started":"2023-06-22T09:37:17.139644Z","shell.execute_reply":"2023-06-22T09:37:17.153883Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                    val\nimgId_eId              \n20057f34d_0    0.058347\n20057f34d_1    0.081329\n20057f34d_2   -0.042682\n20057f34d_3    0.033359\n20057f34d_4    0.017850\n...                 ...\nf27825b2c_379  0.089900\nf27825b2c_380 -0.010372\nf27825b2c_381 -0.007558\nf27825b2c_382 -0.021953\nf27825b2c_383 -0.019022\n\n[2688 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>val</th>\n    </tr>\n    <tr>\n      <th>imgId_eId</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20057f34d_0</th>\n      <td>0.058347</td>\n    </tr>\n    <tr>\n      <th>20057f34d_1</th>\n      <td>0.081329</td>\n    </tr>\n    <tr>\n      <th>20057f34d_2</th>\n      <td>-0.042682</td>\n    </tr>\n    <tr>\n      <th>20057f34d_3</th>\n      <td>0.033359</td>\n    </tr>\n    <tr>\n      <th>20057f34d_4</th>\n      <td>0.017850</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>f27825b2c_379</th>\n      <td>0.089900</td>\n    </tr>\n    <tr>\n      <th>f27825b2c_380</th>\n      <td>-0.010372</td>\n    </tr>\n    <tr>\n      <th>f27825b2c_381</th>\n      <td>-0.007558</td>\n    </tr>\n    <tr>\n      <th>f27825b2c_382</th>\n      <td>-0.021953</td>\n    </tr>\n    <tr>\n      <th>f27825b2c_383</th>\n      <td>-0.019022</td>\n    </tr>\n  </tbody>\n</table>\n<p>2688 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Final File Submission","metadata":{}},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-22T09:37:17.157188Z","iopub.execute_input":"2023-06-22T09:37:17.157769Z","iopub.status.idle":"2023-06-22T09:37:17.170469Z","shell.execute_reply.started":"2023-06-22T09:37:17.157737Z","shell.execute_reply":"2023-06-22T09:37:17.169583Z"},"trusted":true},"execution_count":11,"outputs":[]}]}